"""
RAG Data Provenance - Complete Citation & Source Validation
Ensures 100% citation, confidence scoring, source validation, and provenance tracking
"""

import asyncio
import logging
import hashlib
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

from backend.logging.immutable_log import immutable_log

logger = logging.getLogger(__name__)


class ConfidenceLevel(Enum):
    """Confidence levels for retrieved content"""
    VERY_HIGH = "very_high"    # 0.95-1.0
    HIGH = "high"             # 0.85-0.94
    MEDIUM = "medium"         # 0.70-0.84
    LOW = "low"              # 0.50-0.69
    VERY_LOW = "very_low"    # 0.0-0.49


@dataclass
class DataProvenance:
    """Complete provenance information for a piece of data"""
    data_id: str
    source_url: str
    source_type: str  # 'web', 'document', 'api', 'database', 'generated'
    retrieved_at: str
    retrieved_by: str
    confidence_score: float
    confidence_level: ConfidenceLevel
    citation_chain: List[Dict[str, Any]] = field(default_factory=list)
    validation_status: str = "pending"  # pending, validated, failed
    validation_errors: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "data_id": self.data_id,
            "source_url": self.source_url,
            "source_type": self.source_type,
            "retrieved_at": self.retrieved_at,
            "retrieved_by": self.retrieved_by,
            "confidence_score": self.confidence_score,
            "confidence_level": self.confidence_level.value,
            "citation_chain": self.citation_chain,
            "validation_status": self.validation_status,
            "validation_errors": self.validation_errors,
            "metadata": self.metadata
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DataProvenance':
        return cls(
            data_id=data["data_id"],
            source_url=data["source_url"],
            source_type=data["source_type"],
            retrieved_at=data["retrieved_at"],
            retrieved_by=data["retrieved_by"],
            confidence_score=data["confidence_score"],
            confidence_level=ConfidenceLevel(data["confidence_level"]),
            citation_chain=data.get("citation_chain", []),
            validation_status=data.get("validation_status", "pending"),
            validation_errors=data.get("validation_errors", []),
            metadata=data.get("metadata", {})
        )


class CitationManager:
    """
    Manages citations for all retrieved content
    Ensures 100% citation coverage and proper formatting
    """

    def __init__(self):
        self.citation_templates = {
            "web": "[{author}]({url}) - Retrieved {date}",
            "document": "\"{title}\" from {source} (p. {page})",
            "api": "API: {endpoint} - {provider} ({timestamp})",
            "database": "DB: {table}.{column} - {database} ({query_id})",
            "generated": "Generated by {model} ({confidence:.2f} confidence)"
        }

        self.citation_stats = {
            "total_citations": 0,
            "citations_by_type": {},
            "uncited_responses": 0,
            "citation_format_errors": 0
        }

    def generate_citation(self, provenance: DataProvenance) -> str:
        """
        Generate properly formatted citation from provenance

        Args:
            provenance: Data provenance information

        Returns:
            Formatted citation string
        """
        template = self.citation_templates.get(provenance.source_type, "{source_url}")

        # Format citation based on source type
        if provenance.source_type == "web":
            citation = template.format(
                author=provenance.metadata.get("author", "Unknown"),
                url=provenance.source_url,
                date=provenance.retrieved_at[:10]  # YYYY-MM-DD
            )
        elif provenance.source_type == "document":
            citation = template.format(
                title=provenance.metadata.get("title", "Untitled"),
                source=provenance.metadata.get("source", "Unknown"),
                page=provenance.metadata.get("page", "?")
            )
        elif provenance.source_type == "api":
            citation = template.format(
                endpoint=provenance.metadata.get("endpoint", "unknown"),
                provider=provenance.metadata.get("provider", "unknown"),
                timestamp=provenance.retrieved_at
            )
        elif provenance.source_type == "database":
            citation = template.format(
                table=provenance.metadata.get("table", "unknown"),
                column=provenance.metadata.get("column", "unknown"),
                database=provenance.metadata.get("database", "unknown"),
                query_id=provenance.metadata.get("query_id", "unknown")
            )
        elif provenance.source_type == "generated":
            citation = template.format(
                model=provenance.metadata.get("model", "unknown"),
                confidence=provenance.confidence_score
            )
        else:
            citation = template.format(source_url=provenance.source_url)

        self.citation_stats["total_citations"] += 1
        self.citation_stats["citations_by_type"][provenance.source_type] = \
            self.citation_stats["citations_by_type"].get(provenance.source_type, 0) + 1

        return citation

    def validate_citation_coverage(self, response_text: str, citations: List[str]) -> Dict[str, Any]:
        """
        Validate that response has proper citation coverage

        Args:
            response_text: Generated response text
            citations: List of citations used

        Returns:
            Validation results
        """
        validation = {
            "has_citations": len(citations) > 0,
            "citation_count": len(citations),
            "coverage_percentage": 0.0,
            "issues": []
        }

        if not citations:
            validation["issues"].append("No citations provided")
            self.citation_stats["uncited_responses"] += 1
            return validation

        # Check for citation markers in text ([1], [2], etc.)
        citation_markers = len(re.findall(r'\[\d+\]', response_text))

        if citation_markers == 0:
            validation["issues"].append("No citation markers found in response")
            self.citation_stats["citation_format_errors"] += 1

        # Calculate coverage (simplified - could be enhanced)
        words_in_response = len(response_text.split())
        cited_words_estimate = min(words_in_response, len(citations) * 50)  # Rough estimate
        validation["coverage_percentage"] = (cited_words_estimate / words_in_response) * 100 if words_in_response > 0 else 0

        return validation

    def get_citation_stats(self) -> Dict[str, Any]:
        """Get citation statistics"""
        return self.citation_stats


class ConfidenceScorer:
    """
    Calculates confidence scores for retrieved content
    Considers multiple factors: source reliability, recency, consensus, etc.
    """

    def __init__(self):
        self.source_reliability = {
            "official_docs": 0.95,
            "academic_paper": 0.90,
            "news_article": 0.75,
            "blog_post": 0.60,
            "social_media": 0.40,
            "user_generated": 0.30,
            "generated": 0.70
        }

        self.recency_weights = {
            "hours": 1.0,
            "days": 0.9,
            "weeks": 0.8,
            "months": 0.7,
            "years": 0.5
        }

    def calculate_confidence(self, provenance: DataProvenance,
                           additional_factors: Optional[Dict[str, Any]] = None) -> float:
        """
        Calculate confidence score for retrieved content

        Args:
            provenance: Data provenance information
            additional_factors: Additional scoring factors

        Returns:
            Confidence score (0.0-1.0)
        """
        factors = {
            "source_reliability": self._score_source_reliability(provenance.source_type),
            "recency": self._score_recency(provenance.retrieved_at),
            "validation_status": self._score_validation(provenance.validation_status),
            "citation_chain_length": self._score_citation_chain(provenance.citation_chain)
        }

        # Add additional factors if provided
        if additional_factors:
            factors.update(additional_factors)

        # Weighted average (can be tuned)
        weights = {
            "source_reliability": 0.4,
            "recency": 0.2,
            "validation_status": 0.3,
            "citation_chain_length": 0.1
        }

        confidence = sum(factors[key] * weights[key] for key in weights.keys())

        # Clamp to 0-1 range
        confidence = max(0.0, min(1.0, confidence))

        return confidence

    def _score_source_reliability(self, source_type: str) -> float:
        """Score source reliability"""
        return self.source_reliability.get(source_type, 0.5)

    def _score_recency(self, retrieved_at: str) -> float:
        """Score content recency"""
        try:
            retrieved_time = datetime.fromisoformat(retrieved_at)
            now = datetime.utcnow()
            hours_diff = (now - retrieved_time).total_seconds() / 3600

            if hours_diff < 24:
                return self.recency_weights["hours"]
            elif hours_diff < 168:  # 7 days
                return self.recency_weights["days"]
            elif hours_diff < 720:  # 30 days
                return self.recency_weights["weeks"]
            elif hours_diff < 8760:  # 365 days
                return self.recency_weights["months"]
            else:
                return self.recency_weights["years"]

        except Exception:
            return 0.5  # Default if parsing fails

    def _score_validation(self, validation_status: str) -> float:
        """Score validation status"""
        scores = {
            "validated": 1.0,
            "pending": 0.7,
            "failed": 0.3
        }
        return scores.get(validation_status, 0.5)

    def _score_citation_chain(self, citation_chain: List[Dict[str, Any]]) -> float:
        """Score citation chain length (longer chains = more reliable)"""
        chain_length = len(citation_chain)
        if chain_length == 0:
            return 0.5
        elif chain_length <= 2:
            return 0.7
        elif chain_length <= 5:
            return 0.9
        else:
            return 1.0

    def get_confidence_level(self, score: float) -> ConfidenceLevel:
        """Convert confidence score to level"""
        if score >= 0.95:
            return ConfidenceLevel.VERY_HIGH
        elif score >= 0.85:
            return ConfidenceLevel.HIGH
        elif score >= 0.70:
            return ConfidenceLevel.MEDIUM
        elif score >= 0.50:
            return ConfidenceLevel.LOW
        else:
            return ConfidenceLevel.VERY_LOW


class SourceValidator:
    """
    Validates sources for authenticity and reliability
    Checks URLs, performs fact-checking, validates metadata
    """

    def __init__(self):
        self.validation_cache: Dict[str, Dict[str, Any]] = {}
        self.validation_stats = {
            "total_validated": 0,
            "passed_validation": 0,
            "failed_validation": 0,
            "validation_errors": {}
        }

    async def validate_source(self, provenance: DataProvenance) -> Tuple[bool, List[str]]:
        """
        Validate source authenticity and reliability

        Args:
            provenance: Data provenance to validate

        Returns:
            Tuple of (is_valid, error_messages)
        """
        errors = []

        # Check cache first
        cache_key = f"{provenance.source_url}_{provenance.retrieved_at}"
        if cache_key in self.validation_cache:
            cached = self.validation_cache[cache_key]
            return cached["is_valid"], cached["errors"]

        # Validate based on source type
        if provenance.source_type == "web":
            is_valid, type_errors = await self._validate_web_source(provenance)
            errors.extend(type_errors)
        elif provenance.source_type == "api":
            is_valid, type_errors = await self._validate_api_source(provenance)
            errors.extend(type_errors)
        elif provenance.source_type == "document":
            is_valid, type_errors = await self._validate_document_source(provenance)
            errors.extend(type_errors)
        else:
            is_valid = True  # Default to valid for other types

        # Update provenance
        provenance.validation_status = "validated" if is_valid else "failed"
        provenance.validation_errors = errors

        # Cache result
        self.validation_cache[cache_key] = {
            "is_valid": is_valid,
            "errors": errors,
            "validated_at": datetime.utcnow().isoformat()
        }

        # Update stats
        self.validation_stats["total_validated"] += 1
        if is_valid:
            self.validation_stats["passed_validation"] += 1
        else:
            self.validation_stats["failed_validation"] += 1
            for error in errors:
                self.validation_stats["validation_errors"][error] = \
                    self.validation_stats["validation_errors"].get(error, 0) + 1

        # Log validation
        await immutable_log.append(
            actor="source_validator",
            action="source_validation",
            resource=provenance.data_id,
            outcome="passed" if is_valid else "failed",
            payload={
                "source_url": provenance.source_url,
                "source_type": provenance.source_type,
                "errors": errors
            }
        )

        logger.info(f"[SOURCE-VALIDATOR] Validated {provenance.source_url}: {'PASS' if is_valid else 'FAIL'}")
        return is_valid, errors

    async def _validate_web_source(self, provenance: DataProvenance) -> Tuple[bool, List[str]]:
        """Validate web source"""
        errors = []

        # Basic URL validation
        if not provenance.source_url.startswith(('http://', 'https://')):
            errors.append("Invalid URL format")

        # Check for suspicious patterns
        suspicious_patterns = ['.onion', 'localhost', '127.0.0.1', '0.0.0.0']
        for pattern in suspicious_patterns:
            if pattern in provenance.source_url:
                errors.append(f"Suspicious URL pattern: {pattern}")

        # Could add more validation: SSL cert check, domain reputation, etc.
        # For now, basic validation

        return len(errors) == 0, errors

    async def _validate_api_source(self, provenance: DataProvenance) -> Tuple[bool, List[str]]:
        """Validate API source"""
        errors = []

        # Check required metadata
        required_fields = ["endpoint", "provider"]
        for field in required_fields:
            if field not in provenance.metadata:
                errors.append(f"Missing required API metadata: {field}")

        # Validate endpoint format
        endpoint = provenance.metadata.get("endpoint", "")
        if not endpoint.startswith('/'):
            errors.append("API endpoint should start with '/'")

        return len(errors) == 0, errors

    async def _validate_document_source(self, provenance: DataProvenance) -> Tuple[bool, List[str]]:
        """Validate document source"""
        errors = []

        # Check for required document metadata
        if "title" not in provenance.metadata:
            errors.append("Document missing title metadata")

        # Validate file extension if present
        source_url = provenance.source_url
        if "." in source_url:
            ext = source_url.split(".")[-1].lower()
            valid_exts = ['pdf', 'doc', 'docx', 'txt', 'md', 'html']
            if ext not in valid_exts:
                errors.append(f"Unsupported document extension: {ext}")

        return len(errors) == 0, errors

    def get_validation_stats(self) -> Dict[str, Any]:
        """Get validation statistics"""
        return self.validation_stats

    def clear_cache(self, older_than_hours: int = 24):
        """Clear old validation cache entries"""
        cutoff_time = datetime.utcnow().timestamp() - (older_than_hours * 3600)

        to_remove = []
        for cache_key, cached in self.validation_cache.items():
            try:
                validated_at = datetime.fromisoformat(cached["validated_at"]).timestamp()
                if validated_at < cutoff_time:
                    to_remove.append(cache_key)
            except Exception:
                to_remove.append(cache_key)  # Remove invalid entries

        for key in to_remove:
            del self.validation_cache[key]

        logger.info(f"[SOURCE-VALIDATOR] Cleared {len(to_remove)} old cache entries")


class ProvenanceTracker:
    """
    Tracks complete provenance chain for all data
    Ensures 100% traceability from source to final answer
    """

    def __init__(self):
        self.provenance_store: Dict[str, DataProvenance] = {}
        self.provenance_stats = {
            "total_tracked": 0,
            "by_source_type": {},
            "by_confidence_level": {},
            "validation_coverage": 0.0
        }

    def track_provenance(self, provenance: DataProvenance):
        """Store provenance information"""
        self.provenance_store[provenance.data_id] = provenance

        # Update stats
        self.provenance_stats["total_tracked"] += 1
        self.provenance_stats["by_source_type"][provenance.source_type] = \
            self.provenance_stats["by_source_type"].get(provenance.source_type, 0) + 1
        self.provenance_stats["by_confidence_level"][provenance.confidence_level.value] = \
            self.provenance_stats["by_confidence_level"].get(provenance.confidence_level.value, 0) + 1

        # Calculate validation coverage
        validated_count = sum(1 for p in self.provenance_store.values()
                            if p.validation_status == "validated")
        self.provenance_stats["validation_coverage"] = \
            validated_count / len(self.provenance_store) if self.provenance_store else 0

    def get_provenance(self, data_id: str) -> Optional[DataProvenance]:
        """Retrieve provenance for data ID"""
        return self.provenance_store.get(data_id)

    def get_provenance_chain(self, data_id: str) -> List[DataProvenance]:
        """Get complete provenance chain"""
        chain = []
        current_id = data_id

        # Prevent infinite loops
        visited = set()
        while current_id and current_id not in visited:
            visited.add(current_id)
            provenance = self.get_provenance(current_id)
            if provenance:
                chain.append(provenance)
                # Follow citation chain if available
                if provenance.citation_chain:
                    # Get the first citation's data_id
                    first_citation = provenance.citation_chain[0]
                    current_id = first_citation.get("data_id")
                else:
                    break
            else:
                break

        return chain

    async def validate_provenance_coverage(self, response_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate that response has 100% provenance coverage

        Args:
            response_data: Response data with provenance information

        Returns:
            Validation results
        """
        validation = {
            "coverage_percentage": 0.0,
            "total_statements": 0,
            "cited_statements": 0,
            "missing_citations": [],
            "provenance_chain_complete": True,
            "issues": []
        }

        # Extract statements from response (simplified - would need NLP)
        response_text = response_data.get("response", "")
        statements = self._extract_statements(response_text)
        validation["total_statements"] = len(statements)

        # Check citations for each statement
        citations = response_data.get("citations", [])
        cited_statements = 0

        for statement in statements:
            has_citation = self._statement_has_citation(statement, citations)
            if has_citation:
                cited_statements += 1
            else:
                validation["missing_citations"].append(statement[:100] + "...")

        validation["cited_statements"] = cited_statements
        validation["coverage_percentage"] = \
            (cited_statements / validation["total_statements"]) * 100 if validation["total_statements"] > 0 else 100

        # Check provenance chain completeness
        for citation in citations:
            provenance = self.get_provenance(citation.get("data_id", ""))
            if not provenance:
                validation["provenance_chain_complete"] = False
                validation["issues"].append(f"Missing provenance for citation: {citation}")

        # Overall assessment
        if validation["coverage_percentage"] < 100:
            validation["issues"].append(f"Only {validation['coverage_percentage']:.1f}% citation coverage")

        if not validation["provenance_chain_complete"]:
            validation["issues"].append("Incomplete provenance chains")

        return validation

    def _extract_statements(self, text: str) -> List[str]:
        """Extract statements from text (simplified)"""
        # Split by sentences and filter substantial ones
        import re
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 20]

    def _statement_has_citation(self, statement: str, citations: List[Dict[str, Any]]) -> bool:
        """Check if statement has corresponding citation"""
        # Simplified check - look for citation markers
        return bool(re.search(r'\[\d+\]', statement))

    def get_provenance_stats(self) -> Dict[str, Any]:
        """Get provenance tracking statistics"""
        return self.provenance_stats

    def export_provenance_report(self) -> Dict[str, Any]:
        """Export comprehensive provenance report"""
        return {
            "summary": self.provenance_stats,
            "top_source_types": sorted(
                self.provenance_stats["by_source_type"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5],
            "confidence_distribution": self.provenance_stats["by_confidence_level"],
            "validation_coverage": self.provenance_stats["validation_coverage"],
            "exported_at": datetime.utcnow().isoformat()
        }


# Global instances
citation_manager = CitationManager()
confidence_scorer = ConfidenceScorer()
source_validator = SourceValidator()
provenance_tracker = ProvenanceTracker()