"""
Coding Agent Enhanced API
Adds: Test automation, capability templates, analytics & feedback loops
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
from datetime import datetime

router = APIRouter(prefix="/api/coding_agent", tags=["coding_agent_enhanced"])


class TestResults(BaseModel):
    test_suite: str
    total_tests: int
    passed: int
    failed: int
    skipped: int
    coverage_percent: float
    duration_seconds: float
    failures: List[Dict[str, Any]]


class AnalyticsQuery(BaseModel):
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    project_type: Optional[str] = None
    domain: Optional[str] = None


# ========== CAPABILITY TEMPLATES ==========

CAPABILITY_TEMPLATES = {
    "web_feature": {
        "name": "Web Feature",
        "description": "Build a new feature for a web application",
        "default_domain": "full_stack_web",
        "default_stack": "react_fastapi",
        "phases": [
            {"name": "Requirements & Design", "duration_hours": 1},
            {"name": "Backend API Development", "duration_hours": 2},
            {"name": "Frontend Component Development", "duration_hours": 2},
            {"name": "Integration & Testing", "duration_hours": 1.5},
            {"name": "Documentation", "duration_hours": 0.5}
        ],
        "test_suites": ["unit", "integration", "e2e"],
        "deliverables": [
            "Backend API endpoints",
            "Frontend React components",
            "Database migrations",
            "Unit tests (80%+ coverage)",
            "Integration tests",
            "API documentation",
            "User guide"
        ]
    },
    "infrastructure": {
        "name": "Infrastructure Setup",
        "description": "Provision and configure cloud infrastructure",
        "default_domain": "infrastructure",
        "default_stack": "terraform_aws",
        "phases": [
            {"name": "Requirements Analysis", "duration_hours": 1},
            {"name": "IaC Development (Terraform)", "duration_hours": 3},
            {"name": "Security & Compliance Review", "duration_hours": 1},
            {"name": "Provisioning & Validation", "duration_hours": 2},
            {"name": "Monitoring & Documentation", "duration_hours": 1}
        ],
        "test_suites": ["syntax_validation", "security_scan", "cost_estimation"],
        "deliverables": [
            "Terraform configurations",
            "Kubernetes manifests",
            "Security policies",
            "Monitoring configs",
            "Runbooks",
            "Cost analysis"
        ]
    },
    "research_project": {
        "name": "Research & Analysis",
        "description": "Conduct research and generate insights",
        "default_domain": "research",
        "default_stack": "python_jupyter",
        "phases": [
            {"name": "Data Collection & Preparation", "duration_hours": 2},
            {"name": "Exploratory Analysis", "duration_hours": 3},
            {"name": "Model Development", "duration_hours": 4},
            {"name": "Validation & Reporting", "duration_hours": 2}
        ],
        "test_suites": ["data_validation", "model_evaluation"],
        "deliverables": [
            "Jupyter notebooks",
            "Analysis scripts",
            "Visualizations",
            "Research report",
            "Reproducible pipeline",
            "Datasets (cleaned)"
        ]
    },
    "blockchain_app": {
        "name": "Blockchain Application",
        "description": "Build decentralized application with smart contracts",
        "default_domain": "blockchain",
        "default_stack": "solidity_ethers",
        "phases": [
            {"name": "Smart Contract Design", "duration_hours": 2},
            {"name": "Solidity Development", "duration_hours": 4},
            {"name": "Frontend Web3 Integration", "duration_hours": 3},
            {"name": "Testing (Hardhat/Foundry)", "duration_hours": 2},
            {"name": "Security Audit", "duration_hours": 2},
            {"name": "Deployment to Testnet", "duration_hours": 1}
        ],
        "test_suites": ["unit", "integration", "security_audit", "gas_optimization"],
        "deliverables": [
            "Smart contracts (Solidity)",
            "Web3 frontend (React + Ethers.js)",
            "Test suite (Hardhat)",
            "Security audit report",
            "Deployment scripts",
            "User guide (wallet setup)"
        ]
    },
    "ai_ml_pipeline": {
        "name": "AI/ML Infrastructure",
        "description": "Build AI/ML training and inference pipeline",
        "default_domain": "ai_infrastructure",
        "default_stack": "python_pytorch",
        "phases": [
            {"name": "Data Pipeline Setup", "duration_hours": 3},
            {"name": "Model Architecture", "duration_hours": 4},
            {"name": "Training Infrastructure", "duration_hours": 3},
            {"name": "Inference API", "duration_hours": 2},
            {"name": "Monitoring & MLOps", "duration_hours": 2}
        ],
        "test_suites": ["data_validation", "model_evaluation", "api_integration"],
        "deliverables": [
            "Data preprocessing pipeline",
            "Model training code",
            "Trained model artifacts",
            "Inference API (FastAPI)",
            "MLflow tracking",
            "Monitoring dashboards"
        ]
    },
    "api_service": {
        "name": "API Service",
        "description": "Build RESTful or GraphQL API service",
        "default_domain": "api",
        "default_stack": "fastapi",
        "phases": [
            {"name": "API Design & Spec", "duration_hours": 1},
            {"name": "Endpoint Development", "duration_hours": 3},
            {"name": "Database & Caching", "duration_hours": 2},
            {"name": "Testing & Documentation", "duration_hours": 2},
            {"name": "Deployment & Monitoring", "duration_hours": 1}
        ],
        "test_suites": ["unit", "integration", "load_testing"],
        "deliverables": [
            "API endpoints (RESTful/GraphQL)",
            "OpenAPI specification",
            "Database schema",
            "Caching layer (Redis)",
            "Test suite",
            "API documentation",
            "Deployment config"
        ]
    }
}


@router.get("/templates")
async def get_capability_templates():
    """
    Get all capability templates for common coding tasks
    Used to pre-fill Agentic Builder form
    """
    return {
        "templates": [
            {
                "id": key,
                "name": template["name"],
                "description": template["description"],
                "estimated_hours": sum(p["duration_hours"] for p in template["phases"]),
                "phases_count": len(template["phases"])
            }
            for key, template in CAPABILITY_TEMPLATES.items()
        ]
    }


@router.get("/templates/{template_id}")
async def get_template_details(template_id: str):
    """Get detailed template for pre-filling form"""
    if template_id not in CAPABILITY_TEMPLATES:
        raise HTTPException(status_code=404, detail="Template not found")
    
    return CAPABILITY_TEMPLATES[template_id]


# ========== AUTOMATED TESTING ==========

@router.post("/{intent_id}/run_tests")
async def run_automated_tests(intent_id: str, test_config: Optional[Dict] = None):
    """
    Auto-run test suites after build completion
    Results feed back into co-pilot for fix loops
    """
    # Simulate running tests (in production, this would actually execute tests)
    test_results = {
        "intent_id": intent_id,
        "test_run_id": f"test-run-{datetime.utcnow().timestamp()}",
        "started_at": datetime.utcnow().isoformat(),
        "test_suites": [
            {
                "test_suite": "unit_tests",
                "total_tests": 45,
                "passed": 43,
                "failed": 2,
                "skipped": 0,
                "coverage_percent": 87.5,
                "duration_seconds": 12.3,
                "failures": [
                    {
                        "test_name": "test_websocket_connection",
                        "error_message": "Connection timeout after 5s",
                        "file": "tests/test_chat_api.py",
                        "line": 45
                    },
                    {
                        "test_name": "test_message_persistence",
                        "error_message": "Database constraint violation",
                        "file": "tests/test_chat_api.py",
                        "line": 67
                    }
                ]
            },
            {
                "test_suite": "integration_tests",
                "total_tests": 23,
                "passed": 23,
                "failed": 0,
                "skipped": 0,
                "coverage_percent": 92.1,
                "duration_seconds": 34.7,
                "failures": []
            },
            {
                "test_suite": "e2e_tests",
                "total_tests": 12,
                "passed": 12,
                "failed": 0,
                "skipped": 0,
                "coverage_percent": 78.3,
                "duration_seconds": 89.2,
                "failures": []
            }
        ],
        "completed_at": datetime.utcnow().isoformat(),
        "overall_status": "failed",  # Failed because unit tests have failures
        "total_tests": 80,
        "total_passed": 78,
        "total_failed": 2,
        "overall_coverage": 85.9
    }
    
    # If tests failed, create co-pilot notification for fix loop
    if test_results["overall_status"] == "failed":
        # Would push notification to co-pilot
        notification = {
            "type": "alert",
            "severity": "warning",
            "title": f"Tests Failed for {intent_id}",
            "message": f"2 tests failed. Grace can auto-fix or you can review.",
            "actions": [
                {"label": "üîß Auto-Fix", "action": "auto_fix_tests", "params": {"intent_id": intent_id}},
                {"label": "üëÅ Review Failures", "action": "view_test_failures", "params": {"intent_id": intent_id}},
                {"label": "‚è≠ Deploy Anyway", "action": "deploy_with_failures", "params": {"intent_id": intent_id}}
            ]
        }
    
    return test_results


@router.post("/{intent_id}/fix_tests")
async def auto_fix_test_failures(intent_id: str):
    """
    Automatically fix failed tests
    Re-runs coding agent with test failure context
    """
    # Get test failures
    # Feed back to coding agent
    # Generate fixes
    # Re-run tests
    
    return {
        "intent_id": intent_id,
        "status": "fixing",
        "message": "Coding agent analyzing test failures and generating fixes...",
        "estimated_fix_time_minutes": 15
    }


# ========== ANALYTICS & METRICS ==========

@router.get("/analytics/overview")
async def get_coding_analytics(query: Optional[AnalyticsQuery] = None):
    """
    Get comprehensive analytics for coding agent performance
    Used for tracking improvements and identifying issues
    """
    # Mock data for MVP (in production, query from database)
    analytics = {
        "time_period": {
            "start": query.start_date if query else "2025-11-01",
            "end": query.end_date if query else "2025-11-14",
            "days": 14
        },
        "summary": {
            "total_builds": 15,
            "completed": 14,
            "failed": 1,
            "in_progress": 2,
            "success_rate_percent": 93.3
        },
        "time_metrics": {
            "avg_planned_hours": 8.2,
            "avg_actual_hours": 5.4,
            "avg_efficiency_gain_percent": 34.1,
            "fastest_build_hours": 2.1,
            "slowest_build_hours": 12.3
        },
        "quality_metrics": {
            "avg_test_coverage_percent": 87.5,
            "avg_code_quality_score": "A",
            "builds_with_zero_bugs": 11,
            "avg_rework_cycles": 1.2
        },
        "domain_breakdown": [
            {
                "domain": "full_stack_web",
                "builds": 8,
                "avg_duration_hours": 5.2,
                "success_rate": 100,
                "efficiency_gain": 0.42
            },
            {
                "domain": "infrastructure",
                "builds": 4,
                "avg_duration_hours": 6.1,
                "success_rate": 100,
                "efficiency_gain": 0.28
            },
            {
                "domain": "blockchain",
                "builds": 3,
                "avg_duration_hours": 11.5,
                "success_rate": 66.7,
                "efficiency_gain": 0.08
            }
        ],
        "test_metrics": {
            "total_test_runs": 42,
            "first_pass_rate_percent": 78.6,
            "avg_fix_cycles": 1.3,
            "avg_coverage_percent": 87.5
        },
        "deployment_metrics": {
            "total_deployments": 14,
            "successful_deployments": 13,
            "failed_deployments": 1,
            "avg_deployment_time_minutes": 8.5,
            "rollback_rate_percent": 7.1
        },
        "learning_trends": {
            "builds_week_1": 3,
            "builds_week_2": 12,
            "efficiency_trend": "+15% week over week",
            "quality_trend": "stable",
            "speed_trend": "+22% faster"
        }
    }
    
    return analytics


@router.get("/analytics/success_metrics")
async def get_success_metrics():
    """
    Get key success metrics for coding agent
    Displayed in Layer 3 analytics dashboard
    """
    return {
        "time_to_deliver": {
            "avg_hours": 5.4,
            "median_hours": 4.8,
            "p95_hours": 10.2,
            "trend": "improving"  # improving, stable, declining
        },
        "pass_rates": {
            "first_build_pass_rate": 78.6,
            "after_fixes_pass_rate": 95.7,
            "test_pass_rate": 97.5,
            "deployment_success_rate": 92.9
        },
        "rework_counts": {
            "avg_rework_cycles": 1.2,
            "builds_zero_rework": 8,
            "builds_one_rework": 5,
            "builds_multiple_rework": 2,
            "max_rework_cycles": 3
        },
        "quality_scores": {
            "avg_test_coverage": 87.5,
            "avg_code_quality": "A",
            "security_scan_pass_rate": 100,
            "linting_error_rate": 0.03
        },
        "user_satisfaction": {
            "builds_approved_without_changes": 11,
            "builds_requiring_modifications": 4,
            "user_approval_rate": 93.3
        }
    }


@router.get("/analytics/trends")
async def get_performance_trends(days: int = 30):
    """
    Get performance trends over time
    Shows if coding agent is improving
    """
    # Mock trend data (in production, calculate from historical data)
    return {
        "time_period_days": days,
        "trends": {
            "efficiency": {
                "week_1": 0.15,  # 15% faster than estimated
                "week_2": 0.28,
                "week_3": 0.34,
                "week_4": 0.41,
                "trend": "improving",
                "change_percent": +173  # (0.41 - 0.15) / 0.15 * 100
            },
            "quality": {
                "week_1": 82.3,  # test coverage %
                "week_2": 85.1,
                "week_3": 87.5,
                "week_4": 89.2,
                "trend": "improving",
                "change_percent": +8.4
            },
            "speed": {
                "week_1": 7.2,  # avg hours per build
                "week_2": 6.5,
                "week_3": 5.8,
                "week_4": 5.1,
                "trend": "improving",
                "change_percent": -29.2
            },
            "success_rate": {
                "week_1": 85.7,  # % successful builds
                "week_2": 90.0,
                "week_3": 92.3,
                "week_4": 95.2,
                "trend": "improving",
                "change_percent": +11.1
            }
        },
        "insights": [
            "Efficiency improving as agent learns patterns",
            "Test coverage increasing with better templates",
            "Build speed improving with reusable components",
            "Success rate trending toward 95%+"
        ],
        "recommendations": [
            "Continue current approach",
            "Add more domain-specific templates",
            "Invest in test automation improvements"
        ]
    }


@router.get("/analytics/failures")
async def get_failure_analysis():
    """
    Analyze failures to identify patterns and improvements
    """
    return {
        "total_failures": 3,
        "failure_categories": [
            {
                "category": "test_failures",
                "count": 2,
                "avg_fix_time_minutes": 15,
                "examples": [
                    "WebSocket timeout in unit tests",
                    "Database constraint violation"
                ]
            },
            {
                "category": "deployment_failures",
                "count": 1,
                "avg_fix_time_minutes": 30,
                "examples": [
                    "Missing environment variable in production"
                ]
            }
        ],
        "root_causes": [
            {
                "cause": "Insufficient environment setup validation",
                "occurrences": 1,
                "fix": "Added pre-deployment environment check"
            },
            {
                "cause": "Test timeout thresholds too aggressive",
                "occurrences": 2,
                "fix": "Increased WebSocket test timeout to 10s"
            }
        ],
        "improvements_applied": [
            "Added environment validation step to all builds",
            "Increased default test timeouts for network operations",
            "Enhanced error messages for database constraints"
        ]
    }


# ========== FIX LOOP INTEGRATION ==========

@router.post("/{intent_id}/auto_fix")
async def auto_fix_build_issues(intent_id: str, failure_context: Dict):
    """
    Automatically fix build failures
    Creates new mini-build to address specific issues
    """
    issue_type = failure_context.get("type", "test_failure")
    
    fix_plan = {
        "intent_id": intent_id,
        "fix_id": f"fix-{intent_id}-{datetime.utcnow().timestamp()}",
        "issue_type": issue_type,
        "status": "analyzing",
        "phases": [
            {"name": "Analyze Failure", "duration_minutes": 5},
            {"name": "Generate Fix", "duration_minutes": 10},
            {"name": "Apply & Test", "duration_minutes": 5}
        ],
        "estimated_fix_time_minutes": 20,
        "started_at": datetime.utcnow().isoformat()
    }
    
    return {
        "intent_id": intent_id,
        "fix_id": fix_plan["fix_id"],
        "status": "fixing",
        "estimated_time_minutes": 20,
        "message": "Coding agent analyzing failures and generating fixes..."
    }


@router.get("/{intent_id}/fix_status/{fix_id}")
async def get_fix_status(intent_id: str, fix_id: str):
    """Get status of auto-fix attempt"""
    # Mock fix progress
    return {
        "intent_id": intent_id,
        "fix_id": fix_id,
        "status": "completed",
        "fixes_applied": [
            {
                "file": "tests/test_chat_api.py",
                "line": 45,
                "change": "Increased timeout from 5s to 10s",
                "reason": "WebSocket connections need more time in test env"
            },
            {
                "file": "backend/models/chat_message.py",
                "line": 23,
                "change": "Made user_id nullable for system messages",
                "reason": "Database constraint too strict for all message types"
            }
        ],
        "test_results_after_fix": {
            "total_tests": 45,
            "passed": 45,
            "failed": 0,
            "coverage_percent": 88.2
        },
        "message": "All tests now passing! Ready to deploy."
    }


# ========== CAPABILITY TRACKING ==========

@router.get("/capabilities/learned")
async def get_learned_capabilities():
    """
    Get capabilities the coding agent has learned over time
    Shows accumulated knowledge and reusable patterns
    """
    return {
        "total_patterns": 45,
        "capabilities": [
            {
                "category": "web_development",
                "patterns_learned": 18,
                "builds_using_patterns": 8,
                "avg_time_saved_hours": 2.3,
                "examples": [
                    "WebSocket real-time communication",
                    "JWT authentication flow",
                    "React state management patterns"
                ]
            },
            {
                "category": "testing",
                "patterns_learned": 12,
                "builds_using_patterns": 14,
                "avg_time_saved_hours": 1.5,
                "examples": [
                    "FastAPI test client setup",
                    "Mock WebSocket connections",
                    "Database fixtures & cleanup"
                ]
            },
            {
                "category": "infrastructure",
                "patterns_learned": 8,
                "builds_using_patterns": 4,
                "avg_time_saved_hours": 3.2,
                "examples": [
                    "Terraform AWS EKS module",
                    "Prometheus monitoring setup",
                    "Auto-scaling configurations"
                ]
            },
            {
                "category": "deployment",
                "patterns_learned": 7,
                "builds_using_patterns": 13,
                "avg_time_saved_hours": 0.8,
                "examples": [
                    "Docker multi-stage builds",
                    "K8s deployment manifests",
                    "CI/CD pipeline templates"
                ]
            }
        ],
        "reusable_components": [
            {
                "name": "websocket_handler.py",
                "type": "utility",
                "used_in_builds": 5,
                "lines": 234
            },
            {
                "name": "auth_middleware.py",
                "type": "security",
                "used_in_builds": 8,
                "lines": 156
            },
            {
                "name": "ChatWidget.tsx",
                "type": "component",
                "used_in_builds": 3,
                "lines": 289
            }
        ]
    }


@router.get("/analytics/recommendations")
async def get_improvement_recommendations():
    """
    AI-generated recommendations for improving coding agent
    Based on failure patterns and success metrics
    """
    return {
        "recommendations": [
            {
                "priority": "high",
                "category": "testing",
                "recommendation": "Increase default WebSocket test timeouts to 10s",
                "reasoning": "67% of test failures are WebSocket timeouts",
                "expected_impact": "Reduce test failures by 40%",
                "effort": "low"
            },
            {
                "priority": "high",
                "category": "blockchain",
                "recommendation": "Add Solidity security audit phase by default",
                "reasoning": "Blockchain builds have higher failure rate (66.7% vs 100% for web)",
                "expected_impact": "Improve blockchain success rate to 90%+",
                "effort": "medium"
            },
            {
                "priority": "medium",
                "category": "planning",
                "recommendation": "Create infrastructure sub-templates (EKS, ECS, Lambda)",
                "reasoning": "Infrastructure builds vary widely in duration",
                "expected_impact": "Better time estimates, +15% efficiency",
                "effort": "medium"
            },
            {
                "priority": "low",
                "category": "optimization",
                "recommendation": "Parallelize independent phases where possible",
                "reasoning": "Some phases (docs, tests) can run concurrently",
                "expected_impact": "Reduce build time by 10-15%",
                "effort": "high"
            }
        ],
        "auto_applied": [
            "Increased test timeouts (applied to all new builds)",
            "Added environment validation (prevents deployment failures)"
        ],
        "pending_approval": [
            "Add Solidity security audit phase",
            "Create infrastructure sub-templates"
        ]
    }
