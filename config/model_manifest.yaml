# Grace Model Capability Manifest
# Metadata for all 15 models - Grace reads this before selecting models

models:
  # === CONVERSATION MODELS ===
  
  qwen2.5_72b:
    name: "qwen2.5:72b"
    type: conversation
    context_window: 128000
    latency_ms: 3000
    vram_gb: 48
    specialties:
      - reasoning
      - multilingual
      - complex_analysis
      - general_conversation
    safety_mode: balanced
    trust_score: 0.95
    tier: premium
    cost_per_1k: 0  # Free
    warmup_time_s: 15
    
  qwen2.5_32b:
    name: "qwen2.5:32b"
    type: conversation
    context_window: 128000
    latency_ms: 1500
    vram_gb: 24
    specialties:
      - conversation
      - reasoning
      - general_tasks
      - balanced_performance
    safety_mode: balanced
    trust_score: 0.92
    tier: primary
    cost_per_1k: 0
    warmup_time_s: 8
    
  # === CODING MODELS ===
  
  deepseek_coder_v2_16b:
    name: "deepseek-coder-v2:16b"
    type: coding
    context_window: 16000
    latency_ms: 1000
    vram_gb: 12
    specialties:
      - code_generation
      - debugging
      - refactoring
      - code_review
      - fill_in_middle
    safety_mode: technical
    trust_score: 0.98
    tier: primary
    cost_per_1k: 0
    warmup_time_s: 6
    
  granite_code_20b:
    name: "granite-code:20b"
    type: coding
    context_window: 8000
    latency_ms: 1200
    vram_gb: 16
    specialties:
      - enterprise_code
      - security_focused
      - multiple_languages
    safety_mode: enterprise
    trust_score: 0.90
    tier: secondary
    cost_per_1k: 0
    warmup_time_s: 7
    
  codegemma_7b:
    name: "codegemma:7b"
    type: coding
    context_window: 8000
    latency_ms: 600
    vram_gb: 6
    specialties:
      - code_completion
      - quick_snippets
      - documentation
    safety_mode: balanced
    trust_score: 0.85
    tier: fast
    cost_per_1k: 0
    warmup_time_s: 3
    
  # === REASONING MODELS ===
  
  deepseek_r1_70b:
    name: "deepseek-r1:70b"
    type: reasoning
    context_window: 32000
    latency_ms: 4000
    vram_gb: 48
    specialties:
      - complex_reasoning
      - mathematics
      - logic_problems
      - chain_of_thought
      - o1_competitor
    safety_mode: balanced
    trust_score: 0.96
    tier: premium
    cost_per_1k: 0
    warmup_time_s: 18
    shows_thinking: true
    
  wizardlm2:
    name: "wizardlm2:latest"
    type: reasoning
    context_window: 32000
    latency_ms: 2500
    vram_gb: 28
    specialties:
      - academic_writing
      - research
      - scientific_reasoning
    safety_mode: balanced
    trust_score: 0.88
    tier: secondary
    cost_per_1k: 0
    warmup_time_s: 12
    
  # === LONG CONTEXT MODELS ===
  
  kimi:
    name: "kimi:latest"
    type: long_context
    context_window: 128000
    latency_ms: 800
    vram_gb: 4
    specialties:
      - long_conversations
      - document_analysis
      - context_retention
    safety_mode: balanced
    trust_score: 0.87
    tier: primary
    cost_per_1k: 0
    warmup_time_s: 4
    
  command_r_plus:
    name: "command-r-plus:latest"
    type: long_context
    context_window: 128000
    latency_ms: 1800
    vram_gb: 20
    specialties:
      - rag
      - retrieval
      - citations
      - document_qa
    safety_mode: balanced
    trust_score: 0.91
    tier: premium
    cost_per_1k: 0
    warmup_time_s: 10
    supports_citations: true
    
  # === VISION MODELS ===
  
  llava_34b:
    name: "llava:34b"
    type: vision
    context_window: 4000
    latency_ms: 2000
    vram_gb: 24
    specialties:
      - image_analysis
      - visual_qa
      - screenshot_understanding
      - diagram_interpretation
    safety_mode: balanced
    trust_score: 0.89
    tier: primary
    cost_per_1k: 0
    warmup_time_s: 10
    multimodal: true
    
  # === FAST MODELS ===
  
  phi3.5:
    name: "phi3.5:latest"
    type: fast
    context_window: 4000
    latency_ms: 300
    vram_gb: 6
    specialties:
      - quick_responses
      - simple_tasks
      - low_latency
      - watchdog_checks
    safety_mode: balanced
    trust_score: 0.82
    tier: fast
    cost_per_1k: 0
    warmup_time_s: 2
    
  gemma2_9b:
    name: "gemma2:9b"
    type: fast
    context_window: 8000
    latency_ms: 400
    vram_gb: 7
    specialties:
      - fast_chat
      - efficiency
      - general_quick
    safety_mode: balanced
    trust_score: 0.83
    tier: fast
    cost_per_1k: 0
    warmup_time_s: 3
    
  mistral_nemo:
    name: "mistral-nemo:latest"
    type: fast
    context_window: 8000
    latency_ms: 500
    vram_gb: 6
    specialties:
      - balanced_speed
      - general_tasks
    safety_mode: balanced
    trust_score: 0.84
    tier: fast
    cost_per_1k: 0
    warmup_time_s: 3
    
  llama3.2:
    name: "llama3.2:latest"
    type: fast
    context_window: 8000
    latency_ms: 400
    vram_gb: 2
    specialties:
      - fallback
      - lightweight
      - always_available
    safety_mode: balanced
    trust_score: 0.75
    tier: fallback
    cost_per_1k: 0
    warmup_time_s: 2
    
  # === SPECIALIZED MODELS ===
  
  dolphin_mixtral:
    name: "dolphin-mixtral:latest"
    type: conversation
    context_window: 32000
    latency_ms: 1600
    vram_gb: 26
    specialties:
      - uncensored
      - technical_deep_dive
      - unrestricted_analysis
    safety_mode: uncensored
    trust_score: 0.88
    tier: specialized
    cost_per_1k: 0
    warmup_time_s: 9
    requires_governance_approval: true
    flagged_for_review: true
    
  nous_hermes2_mixtral:
    name: "nous-hermes2-mixtral:latest"
    type: conversation
    context_window: 32000
    latency_ms: 1500
    vram_gb: 26
    specialties:
      - instruction_following
      - complex_tasks
      - multi_step_execution
    safety_mode: balanced
    trust_score: 0.90
    tier: secondary
    cost_per_1k: 0
    warmup_time_s: 9

# === ROUTING POLICIES ===

routing_rules:
  coding:
    primary: deepseek_coder_v2_16b
    fallback: [granite_code_20b, codegemma_7b, qwen2.5_32b]
    
  reasoning:
    primary: deepseek_r1_70b
    fallback: [qwen2.5_72b, wizardlm2, qwen2.5_32b]
    
  conversation:
    primary: qwen2.5_32b
    fallback: [qwen2.5_72b, nous_hermes2_mixtral, llama3.2]
    
  rag:
    primary: command_r_plus
    fallback: [kimi, qwen2.5_72b]
    
  vision:
    primary: llava_34b
    fallback: [qwen2.5_72b]  # Qwen can handle some vision tasks
    
  fast:
    primary: phi3.5
    fallback: [gemma2_9b, mistral_nemo, llama3.2]
    
  long_context:
    primary: kimi
    fallback: [command_r_plus, qwen2.5_72b]
    
  uncensored:
    primary: dolphin_mixtral
    fallback: [qwen2.5_72b]
    requires_approval: true

# === WARM CACHE POLICY (For GPU users) ===

cache_tiers:
  always_loaded:
    - qwen2.5_32b      # Primary conversation
    - deepseek_coder_v2_16b  # Primary coding
    - phi3.5           # Fast responses
    
  load_on_demand:
    - qwen2.5_72b
    - deepseek_r1_70b
    - kimi
    - command_r_plus
    - llava_34b
    
  cold_storage:
    - wizardlm2
    - granite_code_20b
    - codegemma_7b
    - dolphin_mixtral
    - nous_hermes2_mixtral

# === GOVERNANCE HOOKS ===

governance:
  uncensored_models:
    - dolphin_mixtral
    approval_required: true
    review_mode: mandatory
    sandbox_required: true
    
  high_vram_models:
    - qwen2.5_72b
    - deepseek_r1_70b
    check_resources: true
    
  experimental_models:
    - wizardlm2
    verification_required: true

# === TELEMETRY CONFIGURATION ===

telemetry:
  log_every_request: true
  track_metrics:
    - model_used
    - task_type
    - latency
    - success_rate
    - user_rating
    - tests_passed
    - review_approval
    
  storage:
    table: model_performance
    retention_days: 90
    
  learning_threshold:
    min_samples: 10  # Need 10 interactions before learning
    confidence_interval: 0.95

# === REINFORCEMENT LEARNING ===

reinforcement:
  approval_weight: +0.05  # Increase trust score on approval
  rejection_weight: -0.03  # Decrease on rejection
  success_weight: +0.02    # Increase on success
  failure_weight: -0.04    # Decrease on failure
  
  auto_update_manifest: true
  min_trust_score: 0.5
  max_trust_score: 1.0

# === SELF-CRITIQUE ===

self_critique:
  enabled: true
  prompt_after_response: "How confident are you in this answer (0-100)? What limitations did you encounter?"
  store_confidence: true
  threshold_for_retry: 60  # If confidence < 60%, try another model
