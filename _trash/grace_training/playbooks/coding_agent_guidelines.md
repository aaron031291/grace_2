# GRACE Coding Agent Guidelines

## Purpose
This document provides guidelines for coding agents operating within the GRACE platform - both for developing coding agents and for agents that write code.

## Coding Agent Capabilities

### 1. Code Generation
**What Coding Agents Can Do**:
- Generate boilerplate code from templates
- Create API clients from OpenAPI specifications
- Generate database models from schemas
- Create test scaffolding
- Build configuration files

**Best Practices**:
- Always validate generated code syntax
- Include comprehensive comments
- Follow project coding standards
- Generate corresponding tests
- Version generated code

**Example Workflow**:
```
Input: OpenAPI Spec → Parser → Code Generator → Validator → Output: API Client
```

### 2. Code Review
**What Coding Agents Can Do**:
- Static code analysis
- Style compliance checking
- Security vulnerability scanning
- Performance anti-pattern detection
- Documentation completeness check

**Review Checklist**:
- [ ] Code follows style guide
- [ ] No security vulnerabilities detected
- [ ] Test coverage > 80%
- [ ] No hardcoded secrets
- [ ] Functions documented
- [ ] Error handling present
- [ ] No performance anti-patterns

### 3. Refactoring
**What Coding Agents Can Do**:
- Extract duplicate code into functions
- Simplify complex conditional logic
- Optimize database queries
- Update deprecated API usage
- Modernize legacy code patterns

**Refactoring Rules**:
- Maintain existing functionality
- Ensure all tests still pass
- Improve code readability
- Reduce complexity
- Document changes

### 4. Test Generation
**What Coding Agents Can Do**:
- Generate unit tests from function signatures
- Create integration test scaffolds
- Generate test data/fixtures
- Create mocks and stubs
- Generate property-based tests

**Test Quality Standards**:
- Cover happy path and edge cases
- Include negative test cases
- Test error handling
- Use meaningful test names
- Follow AAA pattern (Arrange, Act, Assert)

## Code Generation Standards

### Python
```python
# Agent-generated code template
"""
Module: {module_name}
Generated by: GRACE Coding Agent v{version}
Date: {timestamp}
Purpose: {description}
"""

from typing import Optional, List, Dict
import logging

logger = logging.getLogger(__name__)


class {ClassName}:
    """
    {class_description}
    
    Attributes:
        {attribute_name} ({type}): {description}
    """
    
    def __init__(self, {parameters}):
        """
        Initialize {ClassName}.
        
        Args:
            {param_name} ({type}): {description}
        """
        self.{attribute} = {parameter}
        logger.info(f"Initialized {ClassName}")
    
    def {method_name}(self, {parameters}) -> {return_type}:
        """
        {method_description}
        
        Args:
            {param_name} ({type}): {description}
        
        Returns:
            {return_type}: {description}
        
        Raises:
            {ExceptionType}: {when_raised}
        """
        try:
            # Implementation
            result = self._execute_logic({parameters})
            logger.debug(f"{method_name} completed successfully")
            return result
        except Exception as e:
            logger.error(f"Error in {method_name}: {str(e)}")
            raise
```

### TypeScript
```typescript
/**
 * Module: {module_name}
 * Generated by: GRACE Coding Agent v{version}
 * Date: {timestamp}
 * Purpose: {description}
 */

import { Logger } from './utils/logger';

const logger = new Logger('{module_name}');

/**
 * {interface_description}
 */
export interface {InterfaceName} {
  {propertyName}: {type};
}

/**
 * {class_description}
 */
export class {ClassName} {
  private {propertyName}: {type};

  /**
   * Creates an instance of {ClassName}.
   * @param {param_name} - {description}
   */
  constructor({parameters}: {InterfaceName}) {
    this.{propertyName} = {parameter};
    logger.info(`Initialized ${ClassName}`);
  }

  /**
   * {method_description}
   * @param {param_name} - {description}
   * @returns {description}
   * @throws {ErrorType} {when_thrown}
   */
  public async {methodName}({parameters}): Promise<{ReturnType}> {
    try {
      const result = await this.executeLogic({parameters});
      logger.debug(`${methodName} completed successfully`);
      return result;
    } catch (error) {
      logger.error(`Error in ${methodName}: ${error.message}`);
      throw error;
    }
  }
}
```

## Code Quality Checks

### Automated Checks
```yaml
quality_gates:
  - name: "syntax_validation"
    tools: ["pylint", "eslint", "tsc"]
    pass_threshold: "0 errors"
  
  - name: "security_scan"
    tools: ["bandit", "semgrep", "npm audit"]
    pass_threshold: "0 high, 0 critical"
  
  - name: "test_coverage"
    tools: ["pytest-cov", "jest"]
    pass_threshold: "> 80%"
  
  - name: "complexity"
    tools: ["radon", "complexity-report"]
    pass_threshold: "< 10 cyclomatic complexity"
  
  - name: "documentation"
    tools: ["pydocstyle", "tsdoc"]
    pass_threshold: "all public methods documented"
```

### Manual Review Triggers
Code requires human review if:
- Modifying authentication/authorization logic
- Changing database schemas
- Modifying API contracts
- Touching security-sensitive code
- Making infrastructure changes
- Cyclomatic complexity > 15

## Coding Agent Workflows

### Workflow 1: API Client Generation
```yaml
workflow:
  name: "generate_api_client"
  trigger: "openapi_spec_updated"
  
  steps:
    - name: "validate_spec"
      action: "validate_openapi_schema"
    
    - name: "generate_client"
      action: "run_generator"
      generator: "openapi-generator"
      config:
        language: "python"
        output_dir: "generated/clients"
    
    - name: "add_custom_logic"
      action: "inject_helpers"
      templates: ["retry_logic", "auth_handler"]
    
    - name: "generate_tests"
      action: "create_test_suite"
      coverage_target: 90
    
    - name: "quality_check"
      action: "run_quality_gates"
    
    - name: "create_pr"
      action: "submit_pull_request"
      reviewers: ["api-team"]
```

### Workflow 2: Automated Refactoring
```yaml
workflow:
  name: "refactor_legacy_code"
  trigger: "manual"
  
  steps:
    - name: "analyze_code"
      action: "identify_refactoring_opportunities"
      tools: ["complexity_analyzer", "duplicate_detector"]
    
    - name: "prioritize"
      action: "rank_by_impact"
      criteria: ["complexity_reduction", "maintainability_improvement"]
    
    - name: "refactor"
      action: "apply_refactorings"
      max_files_per_pr: 5
    
    - name: "verify_tests"
      action: "run_test_suite"
      must_pass: true
    
    - name: "benchmark_performance"
      action: "compare_performance"
      baseline: "pre_refactor"
    
    - name: "create_pr"
      action: "submit_pull_request"
      title: "Automated Refactoring: {description}"
```

### Workflow 3: Test Generation
```yaml
workflow:
  name: "generate_tests"
  trigger: "new_code_merged_without_tests"
  
  steps:
    - name: "analyze_code"
      action: "parse_functions"
      extract: ["signatures", "docstrings", "complexity"]
    
    - name: "generate_unit_tests"
      action: "create_tests"
      framework: "pytest"
      patterns: ["happy_path", "edge_cases", "error_cases"]
    
    - name: "generate_integration_tests"
      action: "create_integration_tests"
      if: "function_has_external_dependencies"
    
    - name: "generate_fixtures"
      action: "create_test_data"
      source: "production_samples"
      anonymize: true
    
    - name: "run_tests"
      action: "verify_tests_pass"
    
    - name: "measure_coverage"
      action: "calculate_coverage"
      target: 85
```

## Agent Configuration

### Coding Agent Config
```yaml
coding_agent:
  name: "api-client-generator"
  version: "2.0"
  
  capabilities:
    - "generate_code"
    - "run_tests"
    - "create_pr"
  
  languages:
    - python
    - typescript
    - go
  
  frameworks:
    - fastapi
    - express
    - gin
  
  quality_gates:
    min_test_coverage: 0.85
    max_complexity: 10
    security_scan: true
  
  integrations:
    github:
      repo: "grace-platform/api-clients"
      branch_prefix: "auto-generated/"
    
    ci_cd:
      platform: "github-actions"
      run_on_pr: true
  
  approval_rules:
    auto_merge_if:
      - "all_tests_pass"
      - "quality_gates_pass"
      - "no_security_issues"
    require_review_if:
      - "changes_api_contract"
      - "modifies_auth_logic"
```

## Error Handling

### Agent Error Recovery
```python
class CodingAgentError(Exception):
    """Base exception for coding agent errors"""
    pass

class GenerationError(CodingAgentError):
    """Error during code generation"""
    pass

class ValidationError(CodingAgentError):
    """Generated code failed validation"""
    pass

def generate_code_with_recovery(spec):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            code = generate_code(spec)
            validate_code(code)
            return code
        except ValidationError as e:
            if attempt < max_retries - 1:
                logger.warning(f"Validation failed (attempt {attempt + 1}): {e}")
                # Adjust generation parameters
                spec = adjust_spec_for_retry(spec, e)
            else:
                logger.error("Max retries exceeded, manual intervention required")
                create_ticket("code_generation_failed", spec, e)
                raise
```

## Monitoring & Metrics

### Agent Performance Metrics
```yaml
metrics:
  - name: "code_generation_success_rate"
    target: "> 95%"
  
  - name: "generated_code_quality_score"
    target: "> 90"
  
  - name: "test_coverage_of_generated_code"
    target: "> 85%"
  
  - name: "security_issues_in_generated_code"
    target: "0"
  
  - name: "time_to_generate"
    target: "< 5 minutes"
  
  - name: "pr_approval_rate"
    target: "> 90%"
```

### Dashboards
- Code generation pipeline status
- Quality metrics trends
- Security scan results
- Test coverage over time
- Agent execution history

## Safety & Governance

### Code Generation Guardrails
- Never generate code with hardcoded secrets
- Always include input validation
- Generate code with proper error handling
- Include logging statements
- Add security headers for web endpoints
- Use parameterized queries for databases
- Validate all external inputs

### Audit Trail
All coding agent actions are logged:
- Code generation requests
- Generated code (versioned)
- Quality check results
- Security scan findings
- PRs created
- Review outcomes

### Human Oversight
Required for:
- Production deployments
- Security-sensitive code
- API contract changes
- Database migrations
- Infrastructure modifications

## Examples

### Example: Generate REST API Endpoint
```python
def generate_rest_endpoint(spec):
    """
    Generate REST API endpoint from specification
    """
    template = """
@app.{method}('/{path}')
async def {function_name}({parameters}):
    '''
    {description}
    
    Args:
        {param_docs}
    
    Returns:
        {return_docs}
    '''
    try:
        # Validate inputs
        validated_data = validate_{resource}({validation_params})
        
        # Execute business logic
        result = await {service}.{operation}(validated_data)
        
        # Return response
        return {{
            'status': 'success',
            'data': result
        }}
    except ValidationError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f'Error in {function_name}: {{str(e)}}')
        raise HTTPException(status_code=500, detail='Internal server error')
"""
    
    return template.format(**spec)
```

## Resources
- **SDK Documentation**: https://docs.grace-platform.com/coding-agents
- **Code Templates**: https://github.com/grace-platform/code-templates
- **Best Practices**: https://wiki.grace-platform.com/coding-best-practices
